{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e9ad28",
   "metadata": {},
   "source": [
    "# AML Lab Sessional 1 Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd62e2c",
   "metadata": {},
   "source": [
    "### 1. VS Code is a MUST\n",
    "1. You should code in VS Code. Google Colab not allowed\n",
    "\n",
    "### 2. Pre-requisites for lab in VS Code\n",
    "1. Install PDF export support https://saturncloud.io/blog/how-to-export-jupyter-notebook-by-vscode-in-pdf-format/\n",
    "\n",
    "### 3. At the end, export your notebook as html and ipynb \n",
    "1. Open VS Code command palette Shift + Ctrl + P\n",
    "2. Type \"Export Jupyter Notebook\" in the search bar and select \"Export Jupyter Notebook to html\"\n",
    "3. Upload your html AND ipynb here: https://tinyurl.com/y9ptbej2 (In the prompt, put your name correctly else your submission will be rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53215768",
   "metadata": {},
   "source": [
    "### 4. Lab Sessional Summary \n",
    "1. A code template is given to you in lab sesional in this jupyter notebook\n",
    "2. The template will follow a linear sequence of TODOs appropriately labelled with question marks\n",
    "3. You will have to fill the TODO question marks to compile those notebook cells and proceed to next cells\n",
    "4. You can think of the linear sequence of TODOs as a guided thought process.\n",
    "5. Sessional is open book. Google, github, browse product documentation ChatGPT - Do anything you want, except copying others. No discussing among yourselves (Sending questions to your seniors and seeking answers is prohibited). If you are caught carrying out these illegal activities, you will be reported for immediate action. \n",
    "6. You CANNOT replace the TODO code template with some other code copied from stack overflow, ChatGPT etc. All your browsing and search should give you insights into finally how you can fit that into the framework I provide for the thought process of solving the problem. You will have to mandatorily fill the question marks and proceed with the lab problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0921e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082003a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb5407",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "\n",
    "You are provided with diabetes dataset. Your task is:\n",
    "\n",
    "1. Part 1: To perform EDA and prediction without pipeline\n",
    "2. Part 2: To move EDA and kNN prediction into a pipeline\n",
    "3. Part 3: To combine the pipeline and gridsearch to get best K and weights hyperparameters\n",
    "4. Part 4: Save the model as json file, load & use it to do prediction. This part is optional and will be used to accumulate bonus points as a buffer for the semester lab exam if there is any shortfall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fa98f",
   "metadata": {},
   "source": [
    "# Part 1 - Perform EDA and prediction without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1 Load the csv file\n",
    "df = pd.?(?)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5283e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2 Display information about dataframe\n",
    "df.?(verbose=True, show_counts=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3 Check if there are nulls and display their total for each feature\n",
    "df.?().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 4 Check for duplicates\n",
    "dupsSeries = df.?() # Should return a pandas Series with True False for every row\n",
    "\n",
    "# Print the number of duplicates\n",
    "print(f\"Number of duplicates = {dupsSeries.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5 Drop duplicates inplace\n",
    "df.?(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2966934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 6 Display how many rows exist in dataframe after dropping duplicates\n",
    "df.?(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa203e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 7 Identify candidate features for encoding by seeing how many distinct values are present in every column \n",
    "df.?() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 8\n",
    "# Display all possible values that the column named \"Pregnancies\" has in this dataframe\n",
    "df[\"Pregnancies\"].?()\n",
    "\n",
    "# Based on different possible values of Pregnancies, would you choose this feature to be encoded?\n",
    "# State your reason as 1 sentence comment here\n",
    "# ____________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 9 Find if this is an dataset is imbalanced wrt target variable classes\n",
    "zeroClassCount = df[df[\"Outcome\"] == 0][\"Outcome\"].count()\n",
    "zeroClassCount/? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 10 Run this cell to display correlation matrix heat map\n",
    "# Would you consider dropping any feature based on their correlation? \n",
    "# Type your answers here: _______________\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df.corr(),cmap='plasma',fmt='.2g',annot=True,mask=np.triu(df.corr(),+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c113cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 11 Run this cell to see box plots.\n",
    "# Visually examine the box plot and answer this question\n",
    "# How do you identify the 1.5 IQR boundary? \n",
    "# Which two features have most outliers beyond 1.5 IQR?\n",
    "# (Increase the figure size if you want to see larger image)\n",
    "# Put your answer for both questions here\n",
    "# _______________________________________\n",
    "# _______________________________________\n",
    "\n",
    "df.plot(kind=\"box\",subplots=True,figsize=(15,5),title=\"Data with Outliers\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77808594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 12 Did you see any nulls in the dataset? \n",
    "# Ans: ________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But look at how many features have zeros in them\n",
    "# Display zeroes in each column as percentage\n",
    "for col in df.columns:\n",
    "    count = (df[col] == 0).sum()\n",
    "    percentage = (count * 100)/df.shape[0]\n",
    "    print(f'Count of zeros in Column {col} : {count}, percentage 0s: {percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d62ad2",
   "metadata": {},
   "source": [
    "1. After executing the above cell, you found many cells contain 0.\n",
    "2. Some cells should never contain 0. For e.g. Glucose\n",
    "3. Identify the cells that should never contain 0 and replace those 0 with Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ab3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 13 Replace 0 with Nan for features that should never contain 0\n",
    "# Choose 5 columns you want to replace 0 with Nan\n",
    "df[[?,?,?,?,?]] = \\\n",
    "    df[[?,?,?,?,?]].replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 14  Now do null check again to ensure the right featues have Nans. Otherwise go back and fix\n",
    "\n",
    "df.?().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop(\"Outcome\") #Setup target variable\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 15\n",
    "\n",
    "# Answer these questions with short one liner right in this cell as a comment\n",
    "#\n",
    "# 1. What does the pop operation do?\n",
    "#\n",
    "#\n",
    "# 2. Is pop() idempotent or non-idempotent operation?\n",
    "#\n",
    "#\n",
    "# 3. How do you check if an operation is idempotent/non-idempotent?\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8eddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df # Setup independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9034d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fill this out\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 16 Answer these questions\n",
    "# 1. Did you use stratify?\n",
    "#\n",
    "#\n",
    "# 2. If yes, on which column and why? If not, why not?\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b22b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 17 Display all records that have at least one Nan column value\n",
    "# If you cannot get this, you can leave this cell execution and proceed\n",
    "# Subsequent cells do not depend on this. If you cannot solve this, you can proceed to next cell\n",
    "\n",
    "X_train.loc[?, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47de1f8",
   "metadata": {},
   "source": [
    "##### Deciding on Imputation\n",
    "1. Now you should decide which imputation to use - whether SimpleImputer or IterativeImputer\n",
    "2. Instead of following MCAR/MAR path to decide on SimpleImputer or IterativeImputer, you will take a simple alternative\n",
    "3. Use IterativeImputer if a large number of rows have Nan for a column\n",
    "4. Use SimpleImputer if the number of Nans is very small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train.shape={X_train.shape}\")\n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ec6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 18 Based on the results of previous cell which columns will you apply SimpleImputer ?\n",
    "# Select only those columns here \n",
    "\n",
    "X_train_mean_impute = X_train.loc[:,[?,?,?]]\n",
    "X_train_mean_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e10d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 19 Apply SimpleImputer to appropriate columns only\n",
    "\n",
    ".....\n",
    ".....\n",
    "#Fill code above to create SimpleImputer \n",
    "\n",
    "X_train_mean_imputed = mean_imputer.fit_transform(?)\n",
    "\n",
    "# Display first few\n",
    "X_train_mean_imputed[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fededd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 20 Which columns will you apply IterativeImputer ?\n",
    "# Name those columns and provide 2 line short reason\n",
    "# _____________________________\n",
    "\n",
    "X_train.loc[:,[?,?]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fb3c8",
   "metadata": {},
   "source": [
    "Iterative Imputer uses all non null features of the dataset to impute. To apply Iteratative Imputer on the two columns identified above, you should have all other features combined into the dataframe or numpy nmatrix first. \n",
    "\n",
    "You should do the following for that\n",
    "1. Remove the columns of the dataframe that were subjected to mean imputation\n",
    "2. Convert the remaining dataframe into numpy matrix \n",
    "3. Combine the above numpy matrix with mean imputed columns earlier. The \"combining\" two matrices is achieved by concatenating those two numpy matrices - by using an appropriate numpy stacking function\n",
    "4. The stacked numpy matrix is then used for iterative imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 21 Remove the columns of X_train dataframe that were already subjected to mean imputation\n",
    "# Hold on to the rest as a Numpy matrix\n",
    "X_train_set_aside = X_train.drop([?,?,?], axis=1).to_numpy()\n",
    "X_train_set_aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e48dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 22 Concatenate the two numpy matrices by using an appropriate numpy stacking function\n",
    "# Identify which are those two numpy matrices first and use them\n",
    "\n",
    "X_train = np.?stack(........) # this numpy matrix will be the new X_train from prediction\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 23 Perform Iterative Imputation on two columns with Nan data\n",
    "# At this point only two columns will have Nan - SkinThickness and Insulin\n",
    "# Those 2 will be imputed using all other columns \n",
    "# But to make life easy, this is equivalent to performing Iterative Imputation using all other columns\n",
    "\n",
    ".......\n",
    "Add IterativeImputer here and fit and transform appropriate columns\n",
    "\n",
    "\n",
    "X_train_iter_imputed # this variable should hold your imputer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d54b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 24 Fill the question marks to do z transform\n",
    "\n",
    "from sklearn.? import ?\n",
    "\n",
    "scaler = ?()\n",
    "X_train_scaled = scaler.?(?)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dafbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 25 Train the KNN model in this cell\n",
    "\n",
    "from sklearn.? import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 26  Perform all necessary transformation on X_test in this cell to prepare it for predict()\n",
    "# look at all the transformation logic that was performed on X_train and repeat it for X_test here\n",
    "# Put your code here. No template is provided for this section\n",
    "\n",
    ".....\n",
    "....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 27\n",
    "\n",
    "# Write code here to predict using the model and calculate & display the accuracy of your model \n",
    "# You should get an accuracy of around 75%\n",
    "\n",
    "from sklearn.? import accuracy_score\n",
    "\n",
    "# Do prediction using knn here\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bad42d",
   "metadata": {},
   "source": [
    "# Part 2: Data processing and prediction using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d179087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 28 Repeat activities that need to be done outside pipeline\n",
    "# Reading csv, dropping duplicates & other steps that you might have performed - but need to be done outside the pipeline now  \n",
    "\n",
    "df = pd.read_csv(....)\n",
    ".....\n",
    "#Add other code for dropping duplicates etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c58185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 29 Split the data into X and y. Then do train test split here\n",
    "\n",
    "y = df.pop(\"Outcome\")\n",
    "X = df\n",
    "\n",
    "X_train, X_test, y_train, y_test = ?(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a1451",
   "metadata": {},
   "source": [
    "Design a pipeline that looks like following:\n",
    "\n",
    "![Pipeline](https://onedrive.live.com/embed?resid=A5A4158EF1352FCB%211956&authkey=%21AAa4xv7U8X35D44&width=550)\n",
    "\n",
    "1. SimpleImputer performs mean imputation only on the relevant columns\n",
    "2. IdentityTransformer (whose code is given below) is a custom sklearn transformer that performs no operation on the input data. Input data is passed through as is \n",
    "3. The two are wrapped within a ColumnTransformer\n",
    "4. This is followed by sequential execution of IterativeImputer, StandaScaler and the KNN based clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2193171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a No-op transformer. Use this as is\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class IdentityTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X*1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f98d8",
   "metadata": {},
   "source": [
    "Look at the columns of the dataframe and decide which ones need to go to SimpleImputer and which ones should go to IdentityTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecccade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 30 Select the columns for mean imputation\n",
    "# Do not type the names directly. Use indices\n",
    "mean_impute_cols = df.columns[...].to_list()\n",
    "mean_impute_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_impute_col_mask = df.columns.isin(mean_impute_cols)\n",
    "mean_impute_col_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 31 Use the mean_impute_col_mask to select remaining columns from df.columns to be used with IdentityTransformer\n",
    "other_cols = df.columns[?].to_list()\n",
    "other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 32 Build the pipeline using the diagram shown earlier and the details provided along with it\n",
    " \n",
    "......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ed129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 33 Run this cell to verify if indeed your pipeline looks like what was expected\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "<pipeline_var_name>  # Put your pipeline variable name here to display pipeline as diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 34 Train pipeline with data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 35 Do predictions with pipeline and calculate the accuracy score  \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0a19b",
   "metadata": {},
   "source": [
    "# Part 3: Combine pipeline & gridsearch for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a526416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 36. Use GridSearchCV with KFold = 3 for CV and train a model using the pipeline developed earlier \n",
    "\n",
    "# Create KFold here\n",
    "\n",
    "# Define hyperparameters\n",
    "grid_params = {'model__n_neighbors': [2,3,4,5,6], \n",
    "                'model__weights': ['uniform','distance'], \n",
    "                'model__metric': ['euclidean', 'manhattan']}\n",
    "\n",
    "#Create Grid SearchCV object here using KFold object and gridparams\n",
    "grid = .....\n",
    "\n",
    "#Do hyperparameter tuning here with GridSearchCV object\n",
    "fitted_model = grid.?(....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0398f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 37 Display the results for best score, best estimator and best params selected by Cross validation\n",
    "# fitted_model is the result of hyperpaarameter tuning in the previous cell\n",
    "print(fitted_model.best_score_)\n",
    "print(fitted_model.best_estimator_)\n",
    "print(fitted_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5dded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 38 Do predictions with gridsearch and calculate the accuracy score\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e9814",
   "metadata": {},
   "source": [
    "##### Confusion matrix calculation\n",
    " \n",
    "The next 3 cells are meant for providing hints to calculating the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 39 Put your pipeline name in place of question mark\n",
    "<pipeline_var_name>.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 40 Put your pipeline name in place of first question mark \n",
    "# and appropriate value for second question mark to display the model\n",
    "<pipeline_var_name>.named_steps[put name of your knn model in the pipeline here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 41 Put your pipeline name in place of first question mark \n",
    "# and appropriate value for second question mark to display the model\n",
    "vars(<pipeline_var_name>.named_steps[put name of your knn model in the pipeline here])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 42 Write code to display a confusion matrix by filling the question marks\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=?)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=?)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a752f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 43 Interpret the confusion matrix in your own words\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd219375",
   "metadata": {},
   "source": [
    "# Part 4: Save the model as json file, load & use it to do prediction\n",
    "\n",
    "This is optional and will be used to accumulate bonus points as a buffer for the semester lab exam if there is any shortfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00e9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491196a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
